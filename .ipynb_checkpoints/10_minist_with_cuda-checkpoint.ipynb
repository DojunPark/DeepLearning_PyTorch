{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST (with cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 필요한 모듈을 호출해옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) device 지정 & manual seed 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 학습모델에 필요한 parameter를 설정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "epochs = 15\n",
    "batch_size = 100\n",
    "drop_prob = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 학습, 테스트셋을 불러옵니다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=True,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)\n",
    "\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                        train=False,\n",
    "                        transform=transforms.ToTensor(),\n",
    "                        download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) minibatch 학습을 위해 학습셋을 data loader에 담아둡니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) 모델을 순차적으로 구성합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-1) layer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear1 = nn.Linear(28*28, 512, bias=True)\n",
    "linear2 = nn.Linear(512, 512, bias=True)\n",
    "linear3 = nn.Linear(512, 512, bias=True)\n",
    "linear4 = nn.Linear(512, 512, bias=True)\n",
    "linear5 = nn.Linear(512, 10, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-2) batch normalization 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1 = nn.BatchNorm1d(512)\n",
    "bn2 = nn.BatchNorm1d(512)\n",
    "bn3 = nn.BatchNorm1d(512)\n",
    "bn4 = nn.BatchNorm1d(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-3) activation function 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-4) drop out 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = nn.Dropout(p=drop_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-5) xavier initialization 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0147, -0.0003, -0.0210,  ...,  0.0707, -0.0314, -0.0136],\n",
       "        [ 0.0718, -0.0103,  0.0366,  ..., -0.0319,  0.0462,  0.0303],\n",
       "        [ 0.0575, -0.0890, -0.0492,  ...,  0.0100,  0.0807, -0.0359],\n",
       "        ...,\n",
       "        [-0.0403,  0.0531, -0.0981,  ...,  0.0617, -0.0011,  0.0624],\n",
       "        [-0.0896,  0.0671,  0.0815,  ...,  0.0659,  0.1023, -0.0633],\n",
       "        [ 0.1020,  0.0481, -0.0295,  ..., -0.1030, -0.0380, -0.0858]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.xavier_uniform_(linear1.weight)\n",
    "nn.init.xavier_uniform_(linear2.weight)\n",
    "nn.init.xavier_uniform_(linear3.weight)\n",
    "nn.init.xavier_uniform_(linear4.weight)\n",
    "nn.init.xavier_uniform_(linear5.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-6) model 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                     linear2, bn2, relu, dropout,\n",
    "                     linear3, bn3, relu, dropout,\n",
    "                     linear4, bn4, relu, dropout,\n",
    "                     linear5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) 학습 전 loss function과 optimizer를 지정해줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) for문을 돌면서 training을 시작합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch = len(data_loader)\n",
    "model.train()\n",
    "for epoch in range(1, epochs+1):\n",
    "    avg_loss = 0\n",
    "    \n",
    "    for x_train, y_train in data_loader:\n",
    "        x_train = x_train.view(-1, 28*28).to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        \n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        loss = loss_function(prediction, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += loss / total_batch\n",
    "        \n",
    "    print('epoch: {:5d}/{} >>> loss: {:.6f}'.format(epoch, epochs, avg_loss.item()))\n",
    "    \n",
    "print('learning finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) 학습된 모델을 평가합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    x_test = mnist_test.test_data.view(-1, 28*28).float().to(device)\n",
    "    y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model(x_test)\n",
    "    correct = torch.argmax(prediction, 1) == y_test\n",
    "    accuracy = correct.float().mean()\n",
    "    print(accuracy.item())\n",
    "    \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    x_random_data = mnist_test.test_data[r:r+1].view(-1, 28*28).float().to(device)\n",
    "    y_random_data = mnist_test.test_labels[r:r+1].to(device)\n",
    "    \n",
    "    print('Label: ', y_random_data.item())\n",
    "    single_prediction = model(x_random_data)\n",
    "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
    "    \n",
    "    plt.imshow(mnist_test.test_data[r:r+1].view(28,28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1/15 >>> loss: 0.3027\n",
      "epoch:  2/15 >>> loss: 0.1522\n",
      "epoch:  3/15 >>> loss: 0.1161\n",
      "epoch:  4/15 >>> loss: 0.1041\n",
      "epoch:  5/15 >>> loss: 0.0896\n",
      "epoch:  6/15 >>> loss: 0.0797\n",
      "epoch:  7/15 >>> loss: 0.0715\n",
      "epoch:  8/15 >>> loss: 0.0675\n",
      "epoch:  9/15 >>> loss: 0.0605\n",
      "epoch: 10/15 >>> loss: 0.0552\n",
      "epoch: 11/15 >>> loss: 0.0516\n",
      "epoch: 12/15 >>> loss: 0.0481\n",
      "epoch: 13/15 >>> loss: 0.0456\n",
      "epoch: 14/15 >>> loss: 0.0455\n",
      "epoch: 15/15 >>> loss: 0.0408\n",
      "Learning finished\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed(1)    \n",
    "\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 15\n",
    "learning_rate = 1e-3\n",
    "drop_prop = 0.3\n",
    "\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
    "                         train=True,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=False)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
    "                        train=False,\n",
    "                        transform=transforms.ToTensor(),\n",
    "                        download=False)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         drop_last=True)\n",
    "\n",
    "linear1 = nn.Linear(28*28, 512, bias=True)\n",
    "linear2 = nn.Linear(512, 512, bias=True)\n",
    "linear3 = nn.Linear(512, 512, bias=True)\n",
    "linear4 = nn.Linear(512, 512, bias=True)\n",
    "linear5 = nn.Linear(512, 10, bias=True)\n",
    "\n",
    "bn1 = nn.BatchNorm1d(512)\n",
    "bn2 = nn.BatchNorm1d(512)\n",
    "bn3 = nn.BatchNorm1d(512)\n",
    "bn4 = nn.BatchNorm1d(512)\n",
    "bn5 = nn.BatchNorm1d(512)\n",
    "\n",
    "relu = nn.ReLU()\n",
    "dropout = nn.Dropout(p=drop_prop)\n",
    "\n",
    "nn.init.xavier_uniform_(linear1.weight)\n",
    "nn.init.xavier_uniform_(linear2.weight)\n",
    "nn.init.xavier_uniform_(linear3.weight)\n",
    "nn.init.xavier_uniform_(linear4.weight)\n",
    "nn.init.xavier_uniform_(linear5.weight)\n",
    "\n",
    "model = nn.Sequential(linear1, bn1, relu, dropout,\n",
    "                     linear2, bn2, relu, dropout,\n",
    "                     linear3, bn3, relu, dropout,\n",
    "                     linear4, bn4, relu, dropout,\n",
    "                     linear5).to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, epochs+1):\n",
    "    avg_loss = 0\n",
    "    total_batch = len(data_loader)\n",
    "    \n",
    "    for x_train, y_train in data_loader:\n",
    "        x_train = x_train.view(-1, 28*28).float().to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        \n",
    "        prediction = model(x_train)\n",
    "        \n",
    "        loss = loss_function(prediction, y_train)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += loss / total_batch\n",
    "    \n",
    "    print('epoch: {:2d}/{} >>> loss: {:.4f}'.format(epoch, epochs, avg_loss.item()))\n",
    "\n",
    "print('Learning finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dojun Park\\AppData\\Local\\Continuum\\anaconda3\\envs\\python36\\lib\\site-packages\\torchvision\\datasets\\mnist.py:58: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "C:\\Users\\Dojun Park\\AppData\\Local\\Continuum\\anaconda3\\envs\\python36\\lib\\site-packages\\torchvision\\datasets\\mnist.py:48: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8733000159263611\n",
      "Label:  4\n",
      "Prediction:  4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    x_test = mnist_test.test_data.view(-1, 28*28).float().to(device)\n",
    "    y_test = mnist_test.test_labels.to(device)\n",
    "    \n",
    "    prediction = model(x_test)\n",
    "    correct = torch.argmax(prediction, 1) == y_test\n",
    "    accuracy = correct.float().mean()\n",
    "    print('accuracy: ', accuracy.item())\n",
    "    \n",
    "    r = random.randint(0, len(mnist_test)-1)\n",
    "    x_test_random = x_test[r].view(-1, 28*28).to(device)\n",
    "    y_test_random = y_test[r].to(device)\n",
    "    prediction_random = model(x_test_random)\n",
    "    \n",
    "    print('Label: ', y_test_random.item())\n",
    "    print('Prediction: ', torch.argmax(prediction_random, 1).item())\n",
    "    \n",
    "    plt.imshow(x_test_random.view(28, 28), cmap='Greys', interpolation='nearest')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
