{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 가장 먼저 모듈을 불러옵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) reproducibility를 위해 manual seed를 고정시킵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ccded0cd30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 이제 학습데이터셋을 지정해줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "x = data[:, :-1]\n",
    "y = data[:, [-1]]\n",
    "\n",
    "x_train = torch.FloatTensor(x)\n",
    "y_train = torch.FloatTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([759, 8])\n",
      "torch.Size([759, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 클래스로 모델을 조직해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) 훈련을 위해 model과 optimizer를 선언해줍시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) 본격 트레이닝을 시작해볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1000번째 출력 >>> w:-0.09,-0.38,-0.14,-0.45,-0.08,-0.17,-0.43,-0.04, b:0.30, loss:0.5924\n",
      " 2000번째 출력 >>> w:-0.31,-0.92,-0.23,-0.54,-0.20,-0.39,-0.49,-0.10, b:0.21, loss:0.5493\n",
      " 3000번째 출력 >>> w:-0.46,-1.33,-0.29,-0.59,-0.26,-0.57,-0.55,-0.12, b:0.16, loss:0.5252\n",
      " 4000번째 출력 >>> w:-0.56,-1.65,-0.31,-0.63,-0.30,-0.73,-0.62,-0.13, b:0.14, loss:0.5106\n",
      " 5000번째 출력 >>> w:-0.63,-1.91,-0.33,-0.66,-0.32,-0.86,-0.67,-0.13, b:0.12, loss:0.5010\n",
      " 6000번째 출력 >>> w:-0.68,-2.12,-0.33,-0.68,-0.33,-0.97,-0.72,-0.12, b:0.12, loss:0.4945\n",
      " 7000번째 출력 >>> w:-0.72,-2.30,-0.32,-0.69,-0.34,-1.08,-0.77,-0.11, b:0.12, loss:0.4897\n",
      " 8000번째 출력 >>> w:-0.75,-2.46,-0.31,-0.70,-0.34,-1.17,-0.81,-0.10, b:0.13, loss:0.4863\n",
      " 9000번째 출력 >>> w:-0.77,-2.59,-0.30,-0.71,-0.33,-1.26,-0.84,-0.09, b:0.13, loss:0.4836\n",
      "10000번째 출력 >>> w:-0.79,-2.70,-0.28,-0.71,-0.33,-1.33,-0.87,-0.08, b:0.14, loss:0.4816\n"
     ]
    }
   ],
   "source": [
    "epochs = 10000\n",
    "for epoch in range(1, epochs+1):\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    loss = F.binary_cross_entropy(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        params = list(model.parameters())\n",
    "        w1 = params[0][0][0].item()\n",
    "        w2 = params[0][0][1].item()\n",
    "        w3 = params[0][0][2].item()\n",
    "        w4 = params[0][0][3].item()\n",
    "        w5 = params[0][0][4].item()\n",
    "        w6 = params[0][0][5].item()\n",
    "        w7 = params[0][0][6].item()\n",
    "        w8 = params[0][0][7].item()\n",
    "        b = params[1].item()\n",
    "        \n",
    "        print('{:5d}번째 출력 >>> w:{:.2f},{:.2f},{:.2f},{:.2f},{:.2f},{:.2f},{:.2f},{:.2f}, b:{:.2f}, loss:{:.4f}'\n",
    "              .format(epoch, w1, w2, w3, w4, w5, w6, w7, w8, b, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) 학습이 완료되었으니 accuracy를 확인해봅시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 76.4163%\n"
     ]
    }
   ],
   "source": [
    "prediction = model(x_train)\n",
    "\n",
    "result = prediction >= torch.FloatTensor([0.5])\n",
    "correct = result.float() == y_train\n",
    "\n",
    "accuracy = correct.sum().item() / len(result)\n",
    "print('accuracy: {:.4f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0번째 출력 >>> w:0.17,-0.16,-0.07,0.16,-0.34,0.21,-0.09,0.17, b:0.07, loss:0.70, accuracy:45.72%\n",
      " 1000번째 출력 >>> w:-0.81,-2.76,-0.27,-0.50,-0.45,-1.32,-0.87,-0.05, b:0.18, loss:0.48, accuracy:76.81%\n",
      " 2000번째 출력 >>> w:-0.87,-3.30,-0.09,-0.59,-0.36,-1.87,-0.99,-0.03, b:0.20, loss:0.47, accuracy:76.94%\n",
      " 3000번째 출력 >>> w:-0.89,-3.48,0.07,-0.60,-0.33,-2.16,-1.01,-0.04, b:0.20, loss:0.47, accuracy:77.21%\n",
      " 4000번째 출력 >>> w:-0.89,-3.54,0.19,-0.59,-0.33,-2.34,-1.00,-0.06, b:0.18, loss:0.47, accuracy:77.08%\n",
      " 5000번째 출력 >>> w:-0.89,-3.57,0.28,-0.57,-0.33,-2.46,-1.00,-0.08, b:0.17, loss:0.47, accuracy:76.94%\n",
      " 6000번째 출력 >>> w:-0.90,-3.58,0.34,-0.56,-0.34,-2.53,-0.99,-0.09, b:0.16, loss:0.47, accuracy:76.68%\n",
      " 7000번째 출력 >>> w:-0.90,-3.58,0.39,-0.54,-0.34,-2.59,-0.99,-0.10, b:0.15, loss:0.47, accuracy:76.81%\n",
      " 8000번째 출력 >>> w:-0.90,-3.58,0.42,-0.53,-0.35,-2.62,-0.99,-0.11, b:0.14, loss:0.47, accuracy:76.94%\n",
      " 9000번째 출력 >>> w:-0.90,-3.58,0.44,-0.53,-0.35,-2.65,-0.99,-0.11, b:0.13, loss:0.47, accuracy:76.94%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "data = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_train = torch.FloatTensor(data[:,:-1])\n",
    "y_train = torch.FloatTensor(data[:, [-1]])\n",
    "\n",
    "class LogisticModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "    \n",
    "model = LogisticModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    loss = F.binary_cross_entropy(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        \n",
    "        params = list(model.parameters())\n",
    "        w1 = params[0][0][0].item()\n",
    "        w2 = params[0][0][1].item()\n",
    "        w3 = params[0][0][2].item()\n",
    "        w4 = params[0][0][3].item()\n",
    "        w5 = params[0][0][4].item()\n",
    "        w6 = params[0][0][5].item()\n",
    "        w7 = params[0][0][6].item()\n",
    "        w8 = params[0][0][7].item()\n",
    "        b = params[1].item()\n",
    "        \n",
    "        result = prediction >= torch.FloatTensor([0.5])\n",
    "        correct = result.float() == y_train\n",
    "        accuracy = correct.sum().item() / len(result)\n",
    "        \n",
    "        print('{:5d}번째 출력 >>> w:{:.2f},{:.2f},{:.2f},{:.2f},{:.2f},{:.2f},{:.2f},{:.2f}, b:{:.2f}, loss:{:.2f}, accuracy:{:.2f}%'.format(epoch, w1, w2, w3, w4, w5, w6, w7, w8, b, loss.item() ,accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
