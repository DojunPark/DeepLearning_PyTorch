{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN - at sentence level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 필요한 모듈을 호출합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) manual seed를 고정시킵니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f813b73430>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 학습할 문장을 변수에 지정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ' life is short, you need python!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) 문자 셋과 딕셔너리를 생성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_char = list(set(sent))\n",
    "dic_char = {char:idx for idx, char in enumerate(set_char)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) hyper parameter를 설정합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_size = len(dic_char)\n",
    "hidden_size = len(dic_char)\n",
    "learning_rate = 1e-3\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) 학습 데이터와 레이블을 지정해줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dojun Park\\AppData\\Local\\Continuum\\anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:6: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "idx_sent = [dic_char[c] for c in sent]\n",
    "\n",
    "x_data = [idx_sent[:-1]]\n",
    "y_data = [idx_sent[1:]]\n",
    "\n",
    "x_onehot = [np.eye(dic_size)[x_data]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 31, 17])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.FloatTensor(x_onehot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) 학습 모델을 설계합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(17, 17, batch_first=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn = nn.RNN(dic_size, hidden_size, batch_first=True)\n",
    "rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) 모델 학습 전 loss function과 optimizer를 선언합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) training을 시작합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:  100] loss: 2.3298, output: iise ii shoott t tdntedot!thoo \n",
      "[epoch:  200] loss: 2.0280, output: iife iy t orty , t nyed iythor \n",
      "[epoch:  300] loss: 1.8284, output: iife is short, y u nyed tython \n",
      "[epoch:  400] loss: 1.6808, output: life is short, you need python!\n",
      "[epoch:  500] loss: 1.5818, output: life is short,oyou need python!\n",
      "[epoch:  600] loss: 1.5177, output: life is short,oyou need python!\n",
      "[epoch:  700] loss: 1.4717, output: life is short,oyou need python!\n",
      "[epoch:  800] loss: 1.4366, output: life is short, you need python!\n",
      "[epoch:  900] loss: 1.4089, output: life is short, you need python!\n"
     ]
    }
   ],
   "source": [
    "rnn.train()\n",
    "for epoch in range(1, epochs):\n",
    "    output, _status = rnn(X)\n",
    "    \n",
    "    loss = criterion(output.view(-1, dic_size), Y.view(-1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    result = output.data.numpy().argmax(axis=2)\n",
    "    str_result = ''.join([set_char[idx] for idx in np.squeeze(result)])\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('[epoch:{:5d}] loss: {:.4f}, output: {}'.format(epoch, loss, str_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 5000] loss:1.5325 output:a lersonglonging for any dream for a long tine reaembles thae dream ft last.\n",
      "[epoch:10000] loss:1.4723 output:a lersonglonging for any dream for a long tine resembles that dream ft last.\n",
      "[epoch:15000] loss:1.4850 output:a leeson longing toneany lream for a long tine resembles that dream at last.\n",
      "[epoch:20000] loss:1.4338 output:a lersonglonging for any dream for a long tine resembles that dream at last.\n",
      "[epoch:25000] loss:1.9502 output:a pem f.pataga a aoa a a ameas aoa a aofg tamd a ata les that ereamtat ta t.\n",
      "[epoch:30000] loss:1.8936 output:a pem f.patagama aor a a dream aoa aml ng tame statama atf.as dream at lamtd\n",
      "[epoch:35000] loss:1.7890 output:a aeg fnpaoa ans for a y dr at for a anng nine ftams les that dream at last.\n",
      "[epoch:40000] loss:1.4671 output:a per fnplos ing for any dream for a long tine resembles that dream at last.\n",
      "[epoch:45000] loss:1.5117 output:a persor don ing for any dream for a long tine resembles that dream at last.\n",
      "[epoch:50000] loss:1.4797 output:a lerson longing for aes mream for a long tine resembles that dream ft last.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "sent = ' a person longing for any dream for a long time resembles that dream at last.'\n",
    "\n",
    "set_char = list(set(sent))\n",
    "dic_char = {c:i for i, c in enumerate(set_char)}\n",
    "\n",
    "dic_size = len(dic_char)\n",
    "hidden_size = len(dic_char)\n",
    "epochs = 50000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "idx_sent = [dic_char[c] for c in sent]\n",
    "x_data = [idx_sent[:-1]]\n",
    "y_data = [idx_sent[1:]]\n",
    "x_onehot = [np.eye(dic_size)[x] for x in x_data]\n",
    "\n",
    "X = torch.FloatTensor(x_onehot)\n",
    "Y = torch.LongTensor(y_data)\n",
    "\n",
    "rnn = nn.RNN(dic_size, hidden_size, batch_first=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "rnn.train()\n",
    "for epoch in range(1, epochs+1):\n",
    "    output, _status = rnn(X)\n",
    "    \n",
    "    loss = criterion(output.view(-1, dic_size), Y.view(-1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    result = output.data.numpy().argmax(axis=2)\n",
    "    str_result = ''.join([set_char[idx] for idx in np.squeeze(result)])\n",
    "    \n",
    "    if epoch % 5000 == 0:\n",
    "        print('[epoch:{:5d}] loss:{:.4f} output:{}'.format(epoch, loss, str_result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
