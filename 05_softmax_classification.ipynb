{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) 필요한 모듈을 불러오겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) reproducibility를 위해 manual seed를 고정시키겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18a1fefdd10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) 학습 데이터셋을 불러오겠습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([101, 16])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "x = data[:, :-1]\n",
    "y = data[:, [-1]].squeeze()\n",
    "\n",
    "x_train = torch.FloatTensor(x)\n",
    "y_train = torch.LongTensor(y)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) y 레이블을 one-hot 인코딩 해줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_one_hot = torch.zeros(len(y_train), 7)\n",
    "y_one_hot = y_one_hot.scatter(1, y_train.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) class로 model을 구성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(16, 7)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) model과 optimizer를 선언합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) train을 시작합시다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dojun Park\\AppData\\Local\\Continuum\\anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3000번째 출력 >>> loss:1.4674, accuracy:73.2673%\n",
      " 6000번째 출력 >>> loss:1.4394, accuracy:73.2673%\n",
      " 9000번째 출력 >>> loss:1.4313, accuracy:73.2673%\n",
      "12000번째 출력 >>> loss:1.4268, accuracy:73.2673%\n",
      "15000번째 출력 >>> loss:1.4232, accuracy:73.2673%\n",
      "18000번째 출력 >>> loss:1.4191, accuracy:73.2673%\n",
      "21000번째 출력 >>> loss:1.4133, accuracy:74.2574%\n",
      "24000번째 출력 >>> loss:1.4050, accuracy:77.2277%\n",
      "27000번째 출력 >>> loss:1.3573, accuracy:83.1683%\n",
      "30000번째 출력 >>> loss:1.3481, accuracy:83.1683%\n"
     ]
    }
   ],
   "source": [
    "epochs = 30000\n",
    "for epoch in range(1, epochs+1):\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    loss = F.cross_entropy(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 3000 == 0:\n",
    "        result = prediction.max(1)\n",
    "        correct = result[1] == y_train\n",
    "        accuracy = correct.sum().item() / len(y_train)\n",
    "        \n",
    "        print('{:5d}번째 출력 >>> loss:{:.4f}, accuracy:{:.4f}%'.format(epoch, loss.item(), accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dojun Park\\AppData\\Local\\Continuum\\anaconda3\\envs\\python36\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3000/30000 >>> loss:1.7093, accuracy:44.55%\n",
      "epoch: 6000/30000 >>> loss:1.6655, accuracy:53.47%\n",
      "epoch: 9000/30000 >>> loss:1.6017, accuracy:73.27%\n",
      "epoch:12000/30000 >>> loss:1.5414, accuracy:73.27%\n",
      "epoch:15000/30000 >>> loss:1.4924, accuracy:81.19%\n",
      "epoch:18000/30000 >>> loss:1.4556, accuracy:81.19%\n",
      "epoch:21000/30000 >>> loss:1.4325, accuracy:81.19%\n",
      "epoch:24000/30000 >>> loss:1.4167, accuracy:81.19%\n",
      "epoch:27000/30000 >>> loss:1.4052, accuracy:81.19%\n",
      "epoch:30000/30000 >>> loss:1.3963, accuracy:81.19%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "data = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
    "x_train = torch.FloatTensor(data[:, :-1])\n",
    "y_train = torch.LongTensor(data[:, [-1]]).squeeze(1)\n",
    "\n",
    "y_one_hot = torch.zeros(len(y_train), 7)\n",
    "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
    "\n",
    "class SoftmaxModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(16, 7)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))\n",
    "    \n",
    "model = SoftmaxModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 30000\n",
    "for epoch in range(1, epochs+1):\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    loss = F.cross_entropy(prediction, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 3000 == 0:\n",
    "        result = prediction.max(1)\n",
    "        correct = result[1] == y_train\n",
    "        accuracy = correct.sum().item() / len(y_train)\n",
    "        \n",
    "        print('epoch:{:5d}/30000 >>> loss:{:.4f}, accuracy:{:.2f}%'.format(epoch, loss.item(), accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
